{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralFM\n",
    "Link: https://www.comp.nus.edu.sg/~xiangnan/papers/sigir17-nfm.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import shutil\n",
    "import math\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "from time import time, gmtime, strftime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import EarlyStoppingHook, export_result\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# configファイルの読み込み\n",
    "config_filename = './config/NeuralFM_config.ini'\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read(config_filename)\n",
    "\n",
    "for key in config['model'].keys():\n",
    "    print(key, config['model'][key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの入力\n",
    "HEADER = ['user_id', 'item_id', 'rating']\n",
    "HEADER_DEFAULTS = [['0'], ['0'], [0]]\n",
    "\n",
    "FEATURE_NAMES = ['user_id', 'item_id']\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE = {\n",
    "  'user_id': int(config['model']['user_bucket_size']),\n",
    "  'item_id' : int(config['model']['item_bucket_size'])\n",
    "  }\n",
    "\n",
    "USED_FEATURE_NAMES = ['user_id', 'item_id', 'rating']\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES =  list(CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE.keys())\n",
    "TARGET = 'rating'\n",
    "TARGET_LABELS = [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding_dim = int(config['model']['user_embedding_dim'])\n",
    "item_embedding_dim = int(config['model']['item_embedding_dim'])\n",
    "    \n",
    "# カラム情報取得\n",
    "categorical_hash_user_raw = \\\n",
    "tf.feature_column.categorical_column_with_hash_bucket('user_id', CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE['user_id'])\n",
    "categorical_hash_item_raw = \\\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('item_id', CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE['item_id'])\n",
    "\n",
    "categorical_hash_user = tf.feature_column.indicator_column(categorical_hash_user_raw)    \n",
    "categorical_hash_item = tf.feature_column.indicator_column(categorical_hash_item_raw)\n",
    "categorical_feature_linear = [categorical_hash_user, categorical_hash_item]\n",
    "    \n",
    "# 後半のEmbeddingパートを作成\n",
    "categorical_feature_user_emb = tf.feature_column.embedding_column(\n",
    "    categorical_column=categorical_hash_user_raw, dimension=user_embedding_dim)\n",
    "categorical_feature_item_emb = tf.feature_column.embedding_column(\n",
    "    categorical_column=categorical_hash_item_raw, dimension=item_embedding_dim)\n",
    "categorical_feature_emb = [categorical_feature_user_emb, categorical_feature_item_emb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'categorical_feature_linear': categorical_feature_linear,\n",
    "    'categorical_feature_emb': categorical_feature_emb,\n",
    "    'hidden_units': [64, 32],\n",
    "    'dropout_prob': float(config['model']['dropout_prob']),\n",
    "    'n_classes': len(TARGET_LABELS)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralFM_fn(features, labels, mode, params):\n",
    "    input_features_linear = tf.feature_column.input_layer(features, params['categorical_feature_linear'])\n",
    "    input_features_dnn = tf.feature_column.input_layer(features, params['categorical_feature_emb'])\n",
    "    \n",
    "    feature_sq_sum = tf.square(tf.reduce_sum(input_features_dnn, axis=1, keepdims=True))\n",
    "    feature_sum_sq = tf.reduce_sum(tf.square(input_features_dnn), axis=1, keepdims=True)\n",
    "    cross_term = 0.5 * tf.subtract(feature_sq_sum, feature_sum_sq)\n",
    "    \n",
    "    for hidden_unit in params['hidden_units']:\n",
    "        layer = tf.layers.dense(cross_term, units=hidden_unit, activation=tf.nn.relu)\n",
    "    layer = tf.nn.dropout(layer, rate=params['dropout_prob'])\n",
    "    logits = tf.layers.dense(layer, params['n_classes'], activation=None)\n",
    "    \n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                               predictions=predicted_classes,\n",
    "                               name='acc_op')\n",
    "    \n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input_config(config, phase):\n",
    "    '''\n",
    "    configをパースする関数（内部使用）\n",
    "    configはいつもの設定ファイル\n",
    "    phaseは{train, eval, predict}のいずれか\n",
    "    batch_sizeやnum_epochsなどint型で入って欲しい変数がstrになってしまいintへの変換が必要となったためこの関数を用意した\n",
    "    '''\n",
    "    filename_pattern = config[phase]['filename_pattern']\n",
    "    batch_size = int(config[phase]['batch_size'])\n",
    "    try:\n",
    "        num_epochs = int(config[phase]['num_epochs'])\n",
    "    except:\n",
    "        num_epochs = 1 # Noneにすると一生止まらない\n",
    "    # これだけは正直共通にしておきたい\n",
    "    skip_header_lines = int(config[phase]['skip_header_lines'])\n",
    "\n",
    "    return filename_pattern, batch_size, num_epochs, skip_header_lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    '''\n",
    "    csvをparseする関数\n",
    "    途中でHEADERやHEADER_DEFAULTSを使用しているのでそれらを定義する関数か何かが必要\n",
    "    （lambdaで使用しているため引数に加えることはできない）\n",
    "    '''\n",
    "    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)\n",
    "    features = dict(zip(HEADER, columns))\n",
    "\n",
    "    target = features.pop(TARGET)\n",
    "    return features, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_input_fn(config, phase, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    '''\n",
    "    ファイル名のパターンとか学習・評価時の設定は全てconfigに入れておいた方が管理しやすい\n",
    "    （いつ・どういうタイミングで？）\n",
    "    modeは直打ちで渡す方が楽そう（どこで？）\n",
    "    '''\n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    filename_pattern, batch_size, num_epochs, skip_header_lines = parse_input_config(config, phase)\n",
    "\n",
    "    # ファイル名のパターンを元にデータの読み込み\n",
    "    file_names = tf.matching_files(filename_pattern)\n",
    "    dataset = tf.data.TextLineDataset(filenames=file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "\n",
    "    # バッチサイズ分だけ切り出しgenerateする\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda csv_row: parse_csv_row(csv_row))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    features, target = iterator.get_next()\n",
    "\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serving_input_fn():\n",
    "    '''\n",
    "    serving用のinput_fn\n",
    "    同じくTARGETやUSED_FEATURE_NAMEを呼び出す関数がここにも必要\n",
    "    '''\n",
    "    receiver_tensor = {}\n",
    "    for feature_name in USED_FEATURE_NAMES:\n",
    "        dtype = tf.float32 if feature_name == TARGET else tf.string\n",
    "        receiver_tensor[feature_name] = tf.placeholder(shape=[None], dtype=dtype)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(receiver_tensor, receiver_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = lambda: csv_input_fn(config=config, \n",
    "                                      phase='train', \n",
    "                                      mode=tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "eval_input_fn = lambda: csv_input_fn(config=config,\n",
    "                                     phase='eval', \n",
    "                                     mode=tf.estimator.ModeKeys.EVAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn,\n",
    "                  max_steps=int(config['train']['max_steps']),\n",
    "                  hooks=[EarlyStoppingHook(int(config['model']['early_stop']))]\n",
    "                  )\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn,\n",
    "                exporters=[tf.estimator.LatestExporter(name=\"estimate\",  \n",
    "                                                       serving_input_receiver_fn=json_serving_input_fn)],\n",
    "                steps=None,\n",
    "                throttle_secs = 15\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_execute_time = gmtime()\n",
    "execute_time = strftime(\"%Y%m%d_%H%M%S\", raw_execute_time )\n",
    "model_dir = os.path.join(config['path']['model_dir'], execute_time)\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig().replace(model_dir=model_dir,save_checkpoints_secs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn=NeuralFM_fn, \n",
    "                                  params=params, \n",
    "                                  config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測&評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(config['predict']['filename_pattern'])\n",
    "test_size = len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = lambda: csv_input_fn(config=config, \n",
    "                                      phase='predict', \n",
    "                                      mode=tf.estimator.ModeKeys.PREDICT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values = list(map(lambda item: item[\"class_ids\"][0],list(itertools.islice(predictions, test_size))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value = np.array(test_data.iloc[:,2])\n",
    "pred_value = np.array(values)\n",
    "pred_value_binary = np.round(pred_value)\n",
    "\n",
    "auc = roc_auc_score(test_value, pred_value)\n",
    "accuracy = accuracy_score(test_value, pred_value_binary)\n",
    "print('AUC: {:.4f}\\nAccuracy: {:.4f}'.format(auc, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の出力\n",
    "model_name = 'NeuralFM'\n",
    "export_result(model_name, auc, accuracy, config_filename, execute_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
